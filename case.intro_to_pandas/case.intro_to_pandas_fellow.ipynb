{
  "metadata": {
    "c1_recart": "7.65.0-57c20131aabc1dc2a8c675852d80a7da",
    "celltoolbar": "Tags",
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Basic DataFrame manipulation in `pandas`",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Introduction\n\nToday we will learn how to manipulate tables in Python using the **`pandas`** library.\n\n`pandas` stands for \"Python Data Analysis\". It is a Python library that facilitates a wide range of data analysis and manipulation. Before, you saw basic data structures in Python such as lists and dictionaries. While you can build a basic data table using nested lists in Python (similar to an Excel spreadsheet), they get quite difficult to work with. By contrast, in `pandas` the table data structure, known as a DataFrame, is a first-class citizen and you can easily manipulate your data thinking of it in rows and columns.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## DataFrames and Series\n\nIf you've ever used or heard of R or SQL before, `pandas` brings some functionality from each of these to Python, allowing you to structure and filter data more efficiently than when using pure Python. This efficiency is seen in two distinct ways:\n\n* Code written using `pandas` will often run faster than scripts written in pure Python\n* Code written using `pandas` will often contain far fewer lines of code than the equivalent code written in pure Python\n\nAt the core of the ```pandas``` library are two fundamental data structures/objects:\n1. [Series](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html)\n2. [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html)\n\nA **```Series```** object stores single-column data along with an **index**. An index is just a way of \"numbering\" the ```Series``` object.\n\nA **```DataFrame```** object is a two-dimensional tabular data structure with labeled axes. It is conceptually helpful to think of a DataFrame object as a collection of Series objects. Namely, think of each column in a DataFrame as a single Series object, where each of these Series objects shares a common index -  the index of the DataFrame object.\n\nThe following diagrams show the differences between a Series and a DataFrame. This is a DataFrame - notice that it has an index for its columns and an index for its rows:  \n\n<img src=\"data/images/dataframe.jpg\" alt=\"A Dataframe\" style=\"width: 500px;\"/>\n\nIf you *slice* a DataFrame to extract just *one* column, you'll get a Series. Series can also be created from scratch, independent of any DataFrame. This is a Series:\n\n<img src=\"data/images/series.jpg\" alt=\"A Series\" style=\"width: 600px;\"/>\n\nSeries don't have column indexes, because they don't have columns. You can, however, give your Series a name.\n\nNote that DataFrame objects can also have a single-column â€“ think of this as a DataFrame consisting of a single Series object (they might contain the same information, but are still distinct objects and are treated as such by `pandas`).",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Loading data into `pandas`\n\nA popular way of working with data is storing it in **Comma-Separated Values (CSV) files** and then reading those files into Python using Pandas. CSVs are plain-text files that store tables using one CSV row per table row and separate the columns using commas (although other characters are possible, like semicolons):\n\n<img src=\"data/images/csv.jpg\" alt=\"A CSV file\" style=\"width: 500px;\"/>\n\nTo load a CSV file into `pandas`, you first import the library (the `pd` alias is customary) and then call the [**`.read_csv()`**](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function like this:\n\n~~~python\nimport pandas as pd\npd.read_csv(\"path/to/file.csv\")\n~~~\n\nLet's read in our dataset:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\ndf = pd.read_csv(\"data/watches.csv\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We can inspect the first 5 rows using [**`.head()`**](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html) (to see the last 5 we would use `.tail()`):",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df.head()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Note: **`.head()`** by default shows the first 5 rows, but if you are interested in checking a different number of rows, you can specify that between the parenthesis. For example, if you want to check the first 15 rows, you can use **`.head(15)`**.\n\nThe dataset that we will be working on is a dataset of 75 watches that are sold online (some of them are new, but most are secondhand). There is data for three stores (\"Watches unlimited\", \"National traders\" and \"Super deals\"). The `engagement` column contains a metric of user engagement in the store website, and the `price` column is the price tag.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<hr>\n\n## Checking the Dataset\nTo check more information about the dataset, we can use **`.info()`**.\nIt gives us information about each column in the DataFrame such as the column name, the number of non-null values in the column, and its data type",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df.info()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "To check the number of columns and rows in the DataFrame, we can use **`.shape`**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df.shape",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**`.shape`** gives the dimension of the DataFrame. The first element of the output is the number of rows and the second element is the number of columns. To make the output clearer, we can specify the number of rows to be equal to **`.shape[0]`** and the number of columns to be equal to **`.shape[1]`**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "rows=df.shape[0]\ncolumns=df.shape[1]\nprint(\"There are {} rows and {} columns\".format(rows,columns))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "One of the most important steps when analyzing a dataset, is checking if null values exist. To check for null values, we can use **`.isnull()`**\n\nThis command will interpret the state of each entry:\n\n- If a specific output is False, it means that the attribute value is not null\n- If it shows True, then the attribute value is null",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df.isnull()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "To get a summary of the null values in the columns of a DataFrame, we can use **`.isnull().any()`**\n\nFor each column, it shows either True or False:\n- False means there is not any null value in the column\n- True means there are null values in the column",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df.isnull().any()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "You can check the columns that are included in the DataFrame using **`.columns`**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df.columns",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "To check a column's values, you can use square brackets and include the name of the column within quotes:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df['condition']",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "You can also check the count of each unique element in a column using **`.value_counts()`**. You start with the DataFrame name, followed by the column name, and **`.value_counts()`** at the end\n\n~~~python\nDataFrameName.ColumneName.value_counts()\n~~~",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df.condition.value_counts()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "<hr>",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Sorting and Filtering\n\n### Sorting\n\nYou can sort the data in a DataFrame in ascending or descending order using **`.sort_values()`**. The sorting is, by default, in ascending order.\n    \nIn the below example, we are trying to show the first 5 rows of the dataset, sorted by the column `price` in descending order *(ascending=False)*.\nThe sorted output will be saved in a new DataFrame named `df_sorted`. It is recommended to save the sorted dataset in a new DataFrame to avoid changing the original DataFrame `df`.\n\n~~~python\nNewDataFrameName = OriginalDataFrameName.sort_values('ColumnName' , ascending = False)\n~~~",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df_sorted = df.sort_values('price', ascending = False)\ndf_sorted.head()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "You can also sort in ascending order by assigning *ascending=True* in **`.sort_values`**\n~~~python\nNewDataFrameName = OriginalDataFrameName.sort_values('ColumnName' , ascending = True)\n~~~",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df_sorted = df.sort_values('price', ascending = True)\ndf_sorted.head()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "If you are willing to sort your dataset based on two or more columns, you can include the columns that you want within square brackets. Regarding the ascending element in **`.sort_values()`**, you can either assign it as a bool or a list of bools.\n\n~~~python\nNewDataFrameName = OriginalDataFrameName.sort_values(['ColumnName1', 'ColumnName2'] , ascending=[bool, bool])\n~~~\nIn the following example, we are sorting the dataset based on the columns `engagement` and `price`, so we included the columns names within square brackets.\nRegarding the ascending element in **`.sort_values()`**, each column we are sorting has a different sorting state. We want to sort the `engagement` column in descending order (ascending=False), and the `price` column in ascending order (ascending=True) resulting in *ascending=[False,True]*\n\nThe sorting will be according to the order of the columns in the square brackets. In other words, the `engagement` column will be sorted first in descending order (ascending=False), then the column `price` will be sorted in ascending order(ascending=True). For example, the `engagement` column is sorted in descending order and it has two values equal to 83.164557; because the column `price` is sorted in ascending order, the first `engagement` entry of value 83.164557 has a `price` of 4144.5 then then second `engagement` entry of value 83.164557 has a `price` of 4216.5.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df_sorted = df.sort_values(['engagement','price'], ascending=[False,True])\ndf_sorted.head(10)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Filtering\n\nWhen using `pandas`, you are able to filter your data based on conditions that serve your target.\nIn the below example, we are filtering the dataset based on a specific filter, where we only want the watches of `model` Lightning bolt.\n\n~~~python\nDataFrameName[DataFrameName['ColumnName'] == 'Condition']\n~~~",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df[df['model'] == 'Lightning bolt']",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Note 1: When trying to filter a dataset, make sure to write the Column Name and Condition exactly as they are, because Python is case-sensitive. For example, if you write in the above cell \"Model\" instead of \"model\", you will get an error because the original name of the column is \"model\". The same goes for the condition, if you specify it as \"lightning bolt\" instead of \"Lightning bolt\" you will get an empty DataFrame because there is not any output that satisfies the condition \"lightning bolt\" \n\nNote 2: In the above example, we did not save the filtered output in a new DataFrame, because filtering does not affect the original DataFrame. However, if you are willing to use the filtered result as a seperate output and apply any of the `pandas` functions to it, you should save it in a new DataFrame as in the below example.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "You can also apply multiple filters on numerical and categorical columns. This can be done using operators between different filters\n~~~python\nNewDataFrameName = OriginalDataFrameName[ (OriginalDataFrameName['ColumnName'] == 'Condition') & (OriginalDataFrameName['ColumnName'] < Condition) ]\n~~~",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df_filtered=df[ (df['condition'] == 'New') & (df['price'] < 600) ]\ndf_filtered.head()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "<hr>",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Aggregating Data\n\n### Statistics\n\nThrough `pandas`, you can obtain detailed statistics to get better insights about your data. You could obtain the mean, median, mode, minimum, maximum, count and sum using different functions.\nIn the following examples, we will be finding statistics of the column `price`.  \n\nTo get the mean of a column, use **`.mean()`**:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df.price.mean()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "To get the median of a column, use **`.median()`**:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df.price.median()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "To get the mode of a column, use **`.mode()`**:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df.price.mode()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Note: The result of the above cell shows a 0 at the beginning, but this 0 represents an index in the answer not the value of the mode. In other words, the mode is 831.0 and the 0 at the beginning is an index for 831.0. If there is another value for the mode (other than 831.0), its index will be 1.  \n\nTo get the maximum of a column, use **`.max()`**:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df.price.max()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "To get the minimum of a column, use **`.min()`**:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df.price.min()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "To get a detailed statistics of the numerical columns in a dataframe, use **`.describe()`**\n    \n**`.describe()`** will give you infromation about:\n  \n- **count**: count of non-empty values in each column\n- **mean**: avergae of the values in each column\n- **std**: standard deviation of the values in each column\n- **min**: minimum value in each column\n- **25%**: 25% percentile. For example, the 25% percentile of the column `price` is 530.25, this means that 25% of the `price`'s column values are 530.25 **or less**.\n- **50%**: 50% percentile. For example, the 50% percentile of the column `price` is 831, this means that 50% of the `price`'s column values are 830 or less\n- **75%**: 75% percentile\n- **max**: maximum value in each column    ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df.describe()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "To get a detailed statistics of categorical columns, use **`.describe(include=[object])`**\n    \n   **`.describe(include=[object])`** will give you infromation about:\n\n\n* **count**: count of non-empty values in each column\n* **unique**: number of unique categories in each column. For example, the column `condition` has 5 unique categories(\"New\", \"Like new\", \"Very Good\", \"Good\", \"Fair\")\n* **top**: The category in the column that was repeated the most (the most frequent)\n* **freq**: frequency of the top category. For example, in the `condition` column, the category \"New\" is the most frequent with frequency = 15\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df.describe(include=[object])",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Grouping your data\n\nGrouping your data by a subset of the variables in the DataFrame is a common task in which `pandas` excels. We use the [**`.groupby()`**](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html) method to do this. You can think of `groupby()` as a `for` loop that goes through the values of some user-specified column(s) and, in each iteration, extracts only those DataFrame rows that match that value in the specified column. The below diagram visualizes a `groupby()` operation over the column `foo`. Since there are only two values in `foo` (`one` and `two`), there are only two resulting groups:  \n\n<img src=\"data/images/groupby.jpg\" alt=\"Groupby\" style=\"width: 500px;\"/>\n\nSource: Modified from [`pandas` docs](https://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html).\n\nWhen you do a group by, you're basically splitting your original DataFrame into smaller chunks, depending on the values of one or more columns.\n\nThe output of this method is a `DataFrameGroupBy` object. This object can be most readily thought of as containing a smaller DataFrame object for every group. Specifically, each item of the object is a tuple, containing the group identifier (in this case, `one` or `two`) and the rows of the DataFrame that correspond to that identifier.\n\n`.groupby()` is most useful when we use it together with **aggregation functions**, which are functions that summarize our data. Examples include `sum()` (take the sum of a , `mean()`, and others. Here we group our Dataframe `df` by the `model` and `condition` variables. Inside, the resulting object has all the data from `df`, partitioned into pieces depending on the values of the two variables we grouped by. We then ask `pandas` to compute the mean price for each category:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "groups = df.groupby([\"model\", \"condition\"]) # Grouping by\ngroups[\"price\"].mean() # You first slice using [\"price\"] and then call the mean function on the grouped Series",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "If you'd like to see the contents of the `DataFrameGroupBy` object as such, you can iterate through it using a `for` loop like this:\n\n~~~python\nfor group in groups:\n    print(group)\n~~~",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Exercise 1\n\nRepeat the group by + aggregation above, only this time group by `model` alone instead of `model` and `condition`.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Answer.**",
      "metadata": {
        "tags": [
          "ans_st"
        ]
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Pivoting\n\n**Pivoting** is basically the same as grouping by + aggregating, with the difference that you can have some of the grouping variables as columns instead of as rows. This is sometimes nicer to look at.\n\nLet's say you wanted to see the average `price` of each `model` per `condition`. Rather than doing the group by + aggregation we had above, we could use pivoting to get something like this (we made `condition` a set of columns this time):\n<br><br>\n\n<table border=\"1\" class=\"dataframe\">  <thead>    <tr>      <th></th>      <th colspan=\"5\" halign=\"left\">price</th>    </tr>    <tr>      <th>condition</th>      <th>Fair</th>      <th>Good</th>      <th>Like new</th>      <th>New</th>      <th>Very Good</th>    </tr>    <tr>      <th>model</th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th>Caracal</th>      <td>2949.5</td>      <td>1744.0</td>      <td>508.0</td>      <td>1818.5</td>      <td>1832.5</td>    </tr>    <tr>      <th>Clepsydra</th>      <td>4151.5</td>      <td>604.0</td>      <td>1934.0</td>      <td>4162.5</td>      <td>1737.0</td>    </tr>    <tr>      <th>Lightning bolt</th>      <td>4235.5</td>      <td>655.0</td>      <td>1739.5</td>      <td>652.5</td>      <td>1783.5</td>    </tr>    <tr>      <th>Sand</th>      <td>4207.5</td>      <td>3006.0</td>      <td>1884.5</td>      <td>543.5</td>      <td>520.0</td>    </tr>    <tr>      <th>Tempo</th>      <td>4189.5</td>      <td>4225.5</td>      <td>3039.5</td>      <td>2965.0</td>      <td>1854.5</td>    </tr>  </tbody></table>\n\nThis is called a **pivot table**.  You may already familiar with pivot tables if you have previously worked a fair amount in Microsoft Excel.\n\nWhen you pivot, you summarize one of your numeric columns by group, where the groups are determined by some of the other columns. Some of those variables can be shown as rows, and some can be shown as columns. In the table above, we found the mean `price` grouping by `model` (shown as rows) and `condition` (shown as columns).\n\nWe create pivot tables in `pandas` using the [**`.pivot_table()`**](https://pandas.pydata.org/docs/reference/api/pandas.pivot_table.html) method, with this syntax:\n\n~~~python\npd.pivot_table(my_df, values=[\"numeric_column\"], index=[\"row_variable\"], columns=[\"column_variable\"])\n~~~\n\n`pandas` defaults to summarizing the numeric column using the `.mean()` function (commonly known as average or arithmetic mean). You can use other aggregation functions as well (for more information, check the [docs](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.pivot_table.html)).",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Exercise 2\n\nRecreate the pivot table above using the syntax we've just explained and the variables from the `df` DataFrame.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Answer.**",
      "metadata": {
        "tags": [
          "ans_st"
        ]
      }
    },
    {
      "cell_type": "markdown",
      "source": "The resulting data is identical to the data we got using `.groupby()`, but looks a lot nicer.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<hr>",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Setting and Removing Indexes\n\nBy default, `pandas` assigns a serial number as the index (that's the unnamed column of integers in bold face in the `df.head()` output below), so setting a custom row index that makes sense for your DataFrame is often a good idea.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df.head()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "You can set the index of a DataFrame with the [**`.set_index()`**](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.set_index.html) method. In the following example, we are assigning the first column of the DataFrame `model` , to be an index for the DataFrame:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df = df.set_index([\"model\"])\ndf.head()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Now the columns used as the indexes are emphasized in bold instead.\n\nYou can reset the index and return back to the original index by applying **`.reset_index()`**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df = df.reset_index()\ndf.head()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "You can index a dataset with multiple columns instead of only one:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df = df.set_index([\"model\",\"store\", \"condition\"])\ndf.head()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Unstacking\n\nLet's have a look at our original dataset:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "It has 75 rows and two columns. If we count the three indexes as columns, that makes 5 columns total.\n\nWe sometimes want to reduce the number of rows without removing any data points, like when we need to paste our table in a report and want it to fit on a single page. Additionally, having `condition` as a single column makes it difficult to locate the values that correspond to each possible condition (\"New\", \"Like new\", etc.). We can address both these issues using something called **unstacking**.\n\nWhen you unstack a DataFrame, you take its innermost index level (in this case, `condition`) and turn it into a column of its own, and the DataFrame gets rearranged accordingly. This has the consequence of reducing the number of rows in your dataset while increasing the number of columns. In other words, when you unstack, you reshape your DataFrame to change it from a *long form* to a *wide form*.\n\nUnstacking in `pandas` is very easy with the [**`.unstack()`**](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.unstack.html) method:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df.unstack()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Now we have 15 rows and 12 columns (before we had 75 rows and 5 columns). Since we also avoided having to repeat the variable names, this unstacked table takes a lot less space and can be read more easily.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Exercise 3\n\nWhat is the difference between pivoting and unstacking?",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Answer.**",
      "metadata": {
        "tags": [
          "ans_st"
        ]
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Stacking\n\n**Stacking** is just the reverse of unstacking. You take a \"wide\" DataFrame and make it \"long\". For instance, here we unstack `df`:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "my_unstacked_df = df.unstack()\nmy_unstacked_df.head()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We can reverse the process using [**`.stack()`**](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.stack.html).",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Exercise 4\n\nStack `my_unstacked_df` and verify that the result is exactly equal to `df` (the original DataFrame).",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Answer.**",
      "metadata": {
        "tags": [
          "ans_st"
        ]
      }
    },
    {
      "cell_type": "markdown",
      "source": "If you are interested in learning more about DataFrame manipulation in `pandas`, we encourage you to study their very convenient [user guide](https://pandas.pydata.org/docs/user_guide/reshaping.html).",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Attribution\n\n\"Reshaping and pivot\" (modified from the original), Pandas Developers, [BSD-3 license](https://github.com/pandas-dev/pandas/blob/master/LICENSE), https://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html.",
      "metadata": {}
    }
  ]
}